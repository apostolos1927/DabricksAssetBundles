resources:
  jobs:
    dll_pipeline:
      name: dll_pipeline
      tasks:
        - task_key: ddl
          notebook_task:
            notebook_path: ${workspace.root_path}/files/notebooks/code/demo_table
            source: WORKSPACE
          job_cluster_key: demo_cluster
      job_clusters:
        - job_cluster_key: demo_cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            spark_conf:
              spark.memory.fraction: 0.8
            node_type_id: Standard_DS3_v2
            autoscale:
              min_workers: 1
              max_workers: 1
            data_security_mode: SINGLE_USER
            runtime_engine: PHOTON
      parameters:
        - name: name_prefix
          default: ${var.name_prefix}
        - name: env_name
          default: ${var.env_name}    
        - name: databricks_version
          default: ${var.databricks_version}
    example1_pipeline:
      name: ${var.name_prefix}_pipeline1
      schedule:
        quartz_cron_expression: 0 0 23 * * ?
        timezone_id: America/New_York
        pause_status: UNPAUSED
      tasks:
        - task_key: example1
          notebook_task:
            notebook_path: ${workspace.root_path}/files/notebooks/code/example1
            source: WORKSPACE
          job_cluster_key: demo_cluster
          libraries:
            - whl: /Workspace/${workspace.root_path}/files/dist/framework-0.0.1-py3-none-any.whl
      job_clusters:
        - job_cluster_key: demo_cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            spark_conf:
              spark.memory.fraction: 0.8
            node_type_id: Standard_DS3_v2
            autoscale:
              min_workers: 1
              max_workers: 1
            data_security_mode: SINGLE_USER
            runtime_engine: PHOTON
      parameters:
        - name: name_prefix
          default: ${var.name_prefix}
        - name: env_name
          default: ${var.env_name}    
        - name: databricks_version
          default: ${var.databricks_version}
    example2_pipeline:
      name: ${var.name_prefix}_pipeline2
      schedule:
        quartz_cron_expression: 0 0 23 * * ?
        timezone_id: America/New_York
        pause_status: UNPAUSED
      tasks:
        - task_key: example2
          notebook_task:
            notebook_path: ${workspace.root_path}/files/notebooks/code/example2
            source: WORKSPACE
          job_cluster_key: demo_cluster
          libraries:
            - whl: /Workspace/${workspace.root_path}/files/dist/framework-0.0.1-py3-none-any.whl
      job_clusters:
        - job_cluster_key: demo_cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            spark_conf:
              spark.memory.fraction: 0.8
            node_type_id: Standard_DS3_v2
            autoscale:
              min_workers: 1
              max_workers: 1
            data_security_mode: SINGLE_USER
            runtime_engine: PHOTON
      parameters:
        - name: name_prefix
          default: ${var.name_prefix}
        - name: env_name
          default: ${var.env_name}    
        - name: databricks_version
          default: ${var.databricks_version}
    combined_pipelines:
      name: combined_pipelines
      tasks:
        - task_key: ddl_job
          run_job_task:
            job_id: ${resources.jobs.dll_pipeline.id}
        - task_key: example1_job
          depends_on:
            - task_key: ddl_job
          run_job_task:
            job_id: ${resources.jobs.example1_pipeline.id}
        - task_key: example2_job
          depends_on:
            - task_key: example1_job
          run_job_task:
            job_id: ${resources.jobs.example2_pipeline.id}